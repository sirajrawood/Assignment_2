[
  {
    "objectID": "RWDMOH001.html#sentiment-analysis",
    "href": "RWDMOH001.html#sentiment-analysis",
    "title": "Assignmnet2",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSentiment analysis is a method in which the emotional context of text can be assessed - for instance, are the words conveying a positive or negative emotion or meaning. This is done using a sentiment lexicon in which a variety of words have been assigned an outcome (dependant on the lexicon) - therefore making this a classification problem. For the data we are using, it can be used to provide some political analysis, seeing the overall tone and emotion of a speech, alluding to the governemnts view of the performance of the country and its outlook.\nAs mentioned, a sentiment lexicon is integral. We have opted to use the bing lexicon. This contains a variety of words and assigns it to have either a positive or negative sentiment. To use this, we have joined it onto the tokenised words - showing the sentiment of each word used by each president for a given speech. We have also set words outside of the lexicon to neutral - the hope is that all or at least most of the emotive words have been captured.\nWe then look at the most frequently used positive and negative words used by each president as well as how sentiment changes over time for the different presidents. To do this we look at the net sentiment of the speeches - the amount of positive words minus the negative words."
  },
  {
    "objectID": "RWDMOH001.html#topic-modelling",
    "href": "RWDMOH001.html#topic-modelling",
    "title": "Assignmnet2",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nTopic modeling is a natural language processing technique used to identify latent topics within a collection of documents. Its primary objective is to uncover the underlying themes or topics that contribute to the content of a large corpus. To do this we will be focusing on using Latent Dirichlet Allocation (LDA). In LDA, each document is considered a mix of various topics, and each topic is a probability distribution over words (a mixutre of words). The model assumes that documents are generated based on this probabilistic process, where words are selected from topics, and the mixture of topics determines the overall content of the document. However the number of topics over which to choose from needs to be set. This can be seen as a shortfall as how the outcome is interpreted is highly dependant on this. Choosing a small number of topics can help to summarise the documents very well but may not be able to identify all the distinct topics, whereas a large number of topics may be able to identify all the unqie topics in the corpus, however it won’t really summarise the overarching theme in the corpus. It may also dilute the topics, making them fairly similar and redundant.\nIn order to do this we make use of the topicmodels and ldatuning packages. Before we can proceed with topic modelling, we need to format the data using cast_dtm(), this creates a DocumentTermMatrix object. In order to perform LDA, the number of topics needs to be chosen. In order to do this we use the FindTopicsNumber(). The output can be seen below:\n\n\n  topics Griffiths2004 CaoJuan2009 Arun2010\n1     10     -722976.5   0.1270956 114.5527\n2      9     -723287.0   0.1398716 115.0858\n3      8     -725535.5   0.1644820 114.6463\n4      7     -727499.8   0.1915490 115.8610\n5      6     -727525.5   0.2231102 114.0160\n6      5     -736207.6   0.2819384 123.5482\n7      4     -748785.9   0.4049098 128.3411\n8      3     -760343.9   0.4735794 137.9660\n9      2     -777203.5   0.4958447 147.4585\n\n\n\n\n\nFrom this plot, it is ideal to choose the number of topics where it begins to plateau, being about 6. However with 6 presidents it is possible the topics may cluster around the individual presidents themselves and not around summarising the speeches too well. Moreover, 2 presidents only gave one speech each. Therefore we will opt for 4 topics instead. Now we use the LDA() function, specifying k=4 for the topics. The outout produces \\(\\beta\\) and \\(\\gamma\\) values where \\(\\beta\\) is a parameter that gives the probability of a topic generating a particular word and \\(\\gamma\\) gives the proportions of topics in a document.\nWe assess the \\(\\beta\\) values for the word-topic probabilities to see which words correspond to which topic, allowing us to make assumptions on what the topic is about. We then assess the \\(\\gamm\\) values for each document to see what the overarching topic is for each speech."
  },
  {
    "objectID": "RWDMOH001.html#sentiment-analysis-1",
    "href": "RWDMOH001.html#sentiment-analysis-1",
    "title": "Assignmnet2",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nThe figure below shows the positive words each president used the most throughout their speeches. From this figure we can see that “regard” appears most often for Mandela, Mbeki and Motlanthe while “support” is most often for Ramaphosa and Zuma. Both deKlerk and Mandela say “freedom” and “peace”. Alluding to the political climate of Apartheid. Mbeki, Ramaphosa and Zuma make use of the word “progress” along with either “empowerment: or”reform” as well, due to all the policies and reforms they implemented.\n\n\n\n\n\nFigure 1: Top 10 Positive Words Said per President.\nFigure 2 below shows the negative words each president used the most throughout their speeches. We can see deKlerk uses “concerns”, most likely to keep addressing the concerns people may have regarding the upcoming change. All presidents (excpet deKlerk) are addressing the crime and corruption in the country, as well as the the “poor” things.\n\n\n\n\n\nFigure 2: Top 10 Negative Words Said per President.\nLastly we also observe the net sentiment over time. It can seen that the general sentiment over the years are all positive except that of deKlerk, being the only negative sentiment speech. We can see that Mbeki shows very high positive sentiments, this was a time where the country was experiencing good growth. The same can be said for Ramaphosa.His speeches are generally positive. For Zuma we can see how it becomes decreasingly positive towards the end of his tenure, possibly due to all the allegations and controversies surrounding him.\n\n\n\n\n\nFigure 3: Net Sentiment per President Over the Years."
  },
  {
    "objectID": "RWDMOH001.html#topic-modelling-1",
    "href": "RWDMOH001.html#topic-modelling-1",
    "title": "Assignmnet2",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nWe first assess the \\(\\beta\\) values for each the word-topic probabilities. The greater the value, the more likely the word is to come from that topic. We can assess the words that tend to make up each topic to get an idea of what the topics are that summarise our documents. This can be seen by the figure below:\n\n\n\n\n\nFigure 4: Words per topic with highest values.\nFrom this we now have an idea of what the different topics comprise of, however not with complete certainty. We will also consider the document-topic probabilities (\\(\\gamma\\) values) before making assumptions based on the meanings of the topics.Figure 5 shows for each president, which topic their speeches mostly belong to. Now using both Figure 4 and Figure 5 we can begin to understand the topics.\n\n\n\n\n\nFigure 5: Gamma Distribution by Topic for Each President.\nFigure 5 above shows for deKlerk his speech topic is between 1 and 2 (not well shown). Mandela is mostly topic 1 and 4, Mbeki fluctuates between 1 and 4, Ramaphosa mainly 2 and Zuma mostly 3. Due to the number of topics and the number of presidents that gave multiple speeches being the same, it can be suggested that the topics found are very closely related to the president that delivered the speech. However, it can be see that the document-topic probabilities do still change for some of the presidents. From Figure 4 and 5, we can come to the following conclusions.\nTopic 1 contains words relating to the state of the country. We see the words “freedom”, “past” and “people” appearing. These topics also relate to speeches from Mbeki, Mandela and deKlerk.\nTopic 2 contains words relating to economic growth. This was the main topics for Ramaphosa and deKlerk based around time of drastic change for the country.\nTopic 3 contains words relating to the development of the countries infrastructure and different sectors. What stands out is the word “honourable” and it can be seen how Zuma’s speeches fall into this topic as well - he used this more often than the other presidents.\nTopic 4 contains words relating to the social development, the programmes and policies put into place. This topic relates to Mandela and Mbeki, who did a lot of work for the country regarding this."
  },
  {
    "objectID": "ChatGPT.html",
    "href": "ChatGPT.html",
    "title": "ChatGPT",
    "section": "",
    "text": "For this assignment we have used ChatGPT for assistance where possible. This is to also assess whether it is indeed capable of doing so and if not, how to prompt it in a way that it does help.\n\nMethods\nOne thing I wasn’t too sure aboiut was the number of topics to use for my LDA model. When asking ChatGPT it provided me with the ldatuning package which I used in the assignment. It also provided an explanation of what the package is and what functions from it I should use. It showed how to run the functions as well. There was an issue regarding this as the syntax was not completely correct. However, it is just a simple fix that anyone competent in R can do.\n\n\nCode\nI used ChatGPT to help with some of the coding, especially when crating different graphs and plots. It does well in providing full output for the ggplot code. The only issue may come when it needs to use something specific to your data. An easy way to over come this is to outline your data frame and what the columns contain. From there it can complete whatever you ask. Another great help was adjusted small things on the graph. It would add whatever small extra detail you need in the code, furthermore, it also provides a detailed explanation of what it is doing.\n\n\nWriting\nThere were times where I wasn’t too sure how to flesh out some parts of the assignment. It was easy to ask ChatGPT for a summary of a certain topic or idea I wished to convey. It provides a good enough answer although care still needs to be taken in making sure everything it provides is correct and/or follows correct grammar.\n\n\nConclusion\nChatGPT is a powerful tool. It is easy to use and provides a quick response. It may not be completely accurate every time however, the fix is not too difficult, being whether you can correct yourself or let it know where it went wrong and asking for it to update its answer. Ultimately one can’t use this mindlessly for more complex tasks, it does need the correct prompts to work well."
  }
]